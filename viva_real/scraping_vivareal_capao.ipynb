{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException, ElementClickInterceptedException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ScraperVivaReal:\n",
    "    wait_time = 5\n",
    "\n",
    "    def __init__(self, url):\n",
    "        # Initializing the webdriver\n",
    "\n",
    "        servico = Service(ChromeDriverManager().install())\n",
    "        self.driver = webdriver.Chrome(service=servico)\n",
    "        self.driver.maximize_window()\n",
    "        self.driver.get(url)\n",
    "        time.sleep(self.wait_time)\n",
    "\n",
    "        # Handling cookies acception\n",
    "        WebDriverWait(self.driver, self.wait_time).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"cookie-notifier-cta\"]'))).click()\n",
    "        time.sleep(self.wait_time/2)\n",
    "\n",
    "    def __scrape_page__(self):\n",
    "        result = []\n",
    "\n",
    "        # Extracting data from the page\n",
    "        try:\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        except WebDriverException:\n",
    "            print('Webdriver was manually quit by the user!') # I configure this exception before adding the option -headless to webdriver\n",
    "            return result\n",
    "\n",
    "        # Finding property cards containing search results\n",
    "        div_list = soup.find_all('div', {'class':'property-card__content'})\n",
    "\n",
    "        # Iterating each card\n",
    "        for d in div_list:\n",
    "\n",
    "            # Extracting info from card\n",
    "            title = d.find('span', {'class': 'property-card__title js-cardLink js-card-title'}).get_text().strip()\n",
    "            complete_address = d.find('span', {'class': 'property-card__address'}).get_text().strip()\n",
    "            area = d.find('span', {'class': 'property-card__detail-value js-property-card-value property-card__detail-area js-property-card-detail-area'}).get_text().strip()\n",
    "            rooms = d.find('li', {'class': 'property-card__detail-item property-card__detail-room js-property-detail-rooms'}).find('span', {'class': 'property-card__detail-value js-property-card-value'}).get_text().strip()\n",
    "            baths = d.find('li', {'class': 'property-card__detail-item property-card__detail-bathroom js-property-detail-bathroom'}).find('span', {'class': 'property-card__detail-value js-property-card-value'}).get_text().strip()\n",
    "            garage = d.find('li', {'class': 'property-card__detail-item property-card__detail-garage js-property-detail-garages'}).find('span', {'class': 'property-card__detail-value js-property-card-value'}).get_text().strip()\n",
    "\n",
    "            # Extracting the price\n",
    "            try:\n",
    "                price = d.find('div', {'class':'property-card__price js-property-card-prices js-property-card__price-small'}).find('p').get_text().strip()\n",
    "            except AttributeError:\n",
    "                price = \"N/I\"\n",
    "\n",
    "            # Splitting the address\n",
    "            add_list = re.split(',|-', complete_address)\n",
    "            add_list = [ item.strip() for item in add_list ]\n",
    "            if len(add_list) == 2:\n",
    "                city, st = add_list\n",
    "                neibhood = 'N/I'\n",
    "                address = 'N/I'\n",
    "                number = 'N/I'\n",
    "            if len(add_list) == 3:\n",
    "                neibhood, city, st = add_list\n",
    "                address = 'N/I'\n",
    "                number = 'N/I'\n",
    "            if len(add_list) == 4:\n",
    "                address, neibhood, city, st = add_list\n",
    "                number = 'N/I'\n",
    "            elif len(add_list) == 5:\n",
    "                address, number, neibhood, city, st = add_list\n",
    "\n",
    "            # Adding the result into a dicionary and appending the dict to a result list\n",
    "            row = { 'Título': title, 'Endereço': address, 'Número': number, 'Bairro': neibhood, 'Cidade': city, 'Estado': st, 'Área': area, 'Quartos': rooms, 'Banheiros': baths, 'Vagas': garage, 'Preço': price }\n",
    "            result.append(row)\n",
    "        return result\n",
    "\n",
    "    def __next_page__(self):\n",
    "        # Finding the \"Next Page\" button element\n",
    "        next_element = self.driver.find_element('xpath', f'//*[@id=\"js-site-main\"]/div[2]/div[1]/section/div[2]/div[2]/div/ul/li[9]/button')\n",
    "\n",
    "        try:\n",
    "            # Trying to click it\n",
    "            next_element.click()\n",
    "            time.sleep(self.wait_time)\n",
    "            return True\n",
    "        # Treating some exceptions (element not found and element not clickable)\n",
    "        except ElementClickInterceptedException:\n",
    "            print('\"Próxima Página\" element is not clickable!')\n",
    "        except NoSuchElementException:\n",
    "            print('\"Próxima Página\" element not found!')\n",
    "        return False\n",
    "\n",
    "    def run(self, output):\n",
    "        has_next = True\n",
    "        final_result = []\n",
    "        # Getting the information!\n",
    "        while has_next:\n",
    "            results = self.__scrape_page__()\n",
    "            final_result.extend(results)\n",
    "            print('Got {} results! Total Found: {}'.format(len(results), len(final_result)))\n",
    "            if len(results) == 0:\n",
    "                break\n",
    "            has_next = self.__next_page__()\n",
    "        # Quitting Firefox\n",
    "        self.driver.quit()\n",
    "        # Exporting results to CSV\n",
    "        df = pd.DataFrame(final_result)\n",
    "        df.to_csv(output, sep=',')\n",
    "\n",
    "S = ScraperVivaReal('https://www.vivareal.com.br/venda/rio-grande-do-sul/capao-da-canoa/apartamento_residencial/?pagina=1')\n",
    "S.run('capao2022_output.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
